{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Let us import relevant packages\n\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, Reshape, MaxPooling2D\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-27T20:47:54.342473Z","iopub.execute_input":"2022-11-27T20:47:54.344879Z","iopub.status.idle":"2022-11-27T20:47:54.366442Z","shell.execute_reply.started":"2022-11-27T20:47:54.344784Z","shell.execute_reply":"2022-11-27T20:47:54.365037Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Defining a plotter for convenience later\ndef plot(X):\n    plt.imshow(X, cmap='gray', vmin=0, vmax=1)\n    plt.axis('off')","metadata":{"execution":{"iopub.status.busy":"2022-11-27T20:47:56.002984Z","iopub.execute_input":"2022-11-27T20:47:56.004237Z","iopub.status.idle":"2022-11-27T20:47:56.009635Z","shell.execute_reply.started":"2022-11-27T20:47:56.004192Z","shell.execute_reply":"2022-11-27T20:47:56.008663Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Loading and splitting data to features and target\ndata = np.load('/kaggle/input/flatland/flatland_train.npz')\nX = data['X']\ny = data['y']\n\ny[y != 0] -= 2    # Correct labels so that triangle is mapped to class 1\nX = X / 255.      # Scale down to range [0, 1]","metadata":{"execution":{"iopub.status.busy":"2022-11-27T20:47:57.483114Z","iopub.execute_input":"2022-11-27T20:47:57.483522Z","iopub.status.idle":"2022-11-27T20:47:58.220832Z","shell.execute_reply.started":"2022-11-27T20:47:57.483484Z","shell.execute_reply":"2022-11-27T20:47:58.219559Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Setting the entire dataset as theee training set\nX_train = X\ny_train = y\n\n# Augmenting training seet with all horizontal and vertical transformations\nX_train = np.concatenate([X_train, X_train[..., ::-1, ::-1], X_train[..., :, ::-1], X_train[..., ::-1, :]])\ny_train = np.tile(y_train,4)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T20:47:59.453228Z","iopub.execute_input":"2022-11-27T20:47:59.453643Z","iopub.status.idle":"2022-11-27T20:47:59.838804Z","shell.execute_reply.started":"2022-11-27T20:47:59.453605Z","shell.execute_reply":"2022-11-27T20:47:59.837570Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Model construction\n\nmodel = Sequential()\nmodel.add(Reshape((50,50,1), input_shape=(50,50)))             # Reshaping input images to convolution argument dims\nmodel.add(Conv2D(5, kernel_size=(7, 7), activation='relu'))    # Applying a set of five (7,7) filters with ReLU\nmodel.add(MaxPooling2D(pool_size=(44, 44)))                    # Finding global maximums of resulting matrices\nmodel.add(Flatten())\nmodel.add(Dense(3, activation='relu'))                         # 3 neuron Dense layer with ReLU\nmodel.add(Dense(5, activation='softmax'))                      # Output to 5 neurons with softmax\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=Adam(0.0007),                          # tested several learning rates, 0.0007 approved\n              metrics=[\"accuracy\"])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-11-27T18:47:29.276792Z","iopub.execute_input":"2022-11-27T18:47:29.277239Z","iopub.status.idle":"2022-11-27T18:47:29.335391Z","shell.execute_reply.started":"2022-11-27T18:47:29.277194Z","shell.execute_reply":"2022-11-27T18:47:29.334252Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nreshape (Reshape)            (None, 50, 50, 1)         0         \n_________________________________________________________________\nconv2d (Conv2D)              (None, 44, 44, 5)         250       \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 1, 1, 5)           0         \n_________________________________________________________________\nflatten (Flatten)            (None, 5)                 0         \n_________________________________________________________________\ndense (Dense)                (None, 3)                 18        \n_________________________________________________________________\ndense_1 (Dense)              (None, 5)                 20        \n=================================================================\nTotal params: 288\nTrainable params: 288\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"After the model construction batch sizes 64, 128 or 256 (if i recall correctly, didn't use 16 or 32 yet) in 16, 32, 64 or 128 epochs. Several sets of fitting **X_train** and then explicitly fitting **X** were performed with different combinations of batch size and epochs.\n\nThis is mainly due to the observation through trial and error, that this allowed the model not to get stuck in a 'pit', budging it to either improve or worsen (I got lucky with directions - had other outcomes as well).\n\n<!-- This is mainly due to the observation through trial and error, that there is 'bad' data that does not seem to ever get predicted correctly (at least at small complexity), suggesting the idea that the model should focus on overfitting its training data -->","metadata":{}},{"cell_type":"code","source":"# Model training\n\nloss = model.fit(X_train, y_train, batch_size=128, epochs=64)\n# loss = model.fit(X, y, batch_size=128, epochs=64)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After this training the model had $98.52\\%$ accuracy on training set and $99.99\\%$ accuracy on both test and adv. test sets.\n\nAfter evaluations (on test and adv. test sets) of different models it appeared that continued training to already trained, seemingly unimproving, models, seemed to actually show improvements on both test and adv. test sets, leading to the idea that a strong overfit is actually preferred.\n\nThis, combined with the observation through trials that hitting over $98.52\\%$ accuracy (at least at this complexity) appears to be impossible, leads to the idea that there is some specific **'bad'** data, that consistently fails to be predicted, independent of the model.\n\nAfter constructing several high accuracy ($\\approx 98.51\\%$) models on the training set and checking the first error, we always arrive at the same image - **X**$[19]$.","metadata":{}},{"cell_type":"code","source":"# Importing the final model to reproduce the 'bad' data for demonstration\nreconstructed_model = keras.models.load_model(\"/kaggle/input/overfiteris/overfiteris_3000.h5\")\n\npred = reconstructed_model.predict(X).argmax(axis=1)\n\n# Printing all 'bad' data (indices)\nX_bad_index = np.nonzero(pred-y)[0]\nprint(X_bad_index)\n\nplot(X[19]) # Displaying first incorrect prediction - 19\n","metadata":{"execution":{"iopub.status.busy":"2022-11-27T20:48:24.413888Z","iopub.execute_input":"2022-11-27T20:48:24.414319Z","iopub.status.idle":"2022-11-27T20:48:26.080012Z","shell.execute_reply.started":"2022-11-27T20:48:24.414283Z","shell.execute_reply":"2022-11-27T20:48:26.078269Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"[  19  399  436  473  505  563  597  619  679  696  756  760 1238 1403\n 1409 1413 1442 1467 1474 1598 1748 1854 1975 2105 2148 2183 2225 2226\n 2281 2345 2419 2601 2676 2721 2748 2752 2813 2978 2980 3029 3092 3110\n 3143 3249 3276 3357 3431 3441 3798 3802 3809 3853 3894 3896 3988 4006\n 4025 4037 4063 4068 4094 4115 4168 4243 4273 4294 4402 4452 4484 4488\n 4645 4659 4759 4804 4816 4907 4980 5034 5045 5056 5061 5083 5130 5333\n 5375 5423 5515 5571 5846 5859 5887 5909 5965 5996 6006 6045 6068 6131\n 6155 6162 6222 6317 6376 6384 6452 6517 6559 6653 6670 6674 6679 6776\n 6942 7008 7093 7107 7129 7220 7338 7490 7557 7575 7601 7655 7806 7908\n 7956 7983 8068 8189 8360 8585 8613 8631 8635 8724 8919 9089 9197 9344\n 9495 9566 9591 9664 9687 9704 9718 9839]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"Then since overfitting seems to do the trick, let us continue training our model on the entire training set excluding the 'bad' data - only on the 'good' data, where we should expect perfect prediction accuracy, mainly aiming to solidify those results (squeezing possible loss reduction).","metadata":{}},{"cell_type":"code","source":"# Excluding the 'bad' data from the entire training set\nX_good = X[np.where(pred == y)[0]]\ny_good = y[np.where(pred == y)[0]]","metadata":{"execution":{"iopub.status.busy":"2022-11-27T20:49:28.119271Z","iopub.execute_input":"2022-11-27T20:49:28.119659Z","iopub.status.idle":"2022-11-27T20:49:28.202659Z","shell.execute_reply.started":"2022-11-27T20:49:28.119629Z","shell.execute_reply":"2022-11-27T20:49:28.201664Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"loss = model.fit(X_good, y_good, batch_size=128, epochs=64)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After only a single training session (sadly don't remember used batch size and epochs) the model managed to reduce loss on **X_good** by a factor of $10^3$.\n\nA quick evaluation on the training, test and adv. test sets yielded perfect accuracy on both test and adv. test, as well as the expected $98.52\\%$ accuracy on the training set.","metadata":{}},{"cell_type":"code","source":"# Saving the developed model\nmodel.save('overfiteris_3000.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final thoughts\n\nThe main trick used that helped reduce parameter count drastically was applying **global max pooling**; however, counterintuitively it also was the main trick to get significant accuracy increases. Initially, before even beginning to work on the model, it seemed like a reasonable idea to try applying **global average pooling**, as regular polygons are uniquely defined by their area when diameter is known. While this seemed to work pretty well, global max pooling gave incredible results.\n\nFrom all the Keras provided optimizers, it seemed like **Adam** was the only one that allowed the accuracy to even approach $98.92\\%$, let alone reach it.\n\nIntermediate activations (all but the last **softmax**), seemed to have thee most effect when trying to squeeze to the maximal accuracy:\n* ReLU was incredibly inconsistent at this scale, veery often getting stuck at bad accuracy, but seemed to have thee most potential;\n* ELU was very consistent at quick accuracy improvements and entered $98+\\%$ accuracy territory almost always, but very rarely got to the extremes.\n\nThe final model, **overfiteris_3000**, boasts a size of $35.9\\text{ kB}$ and $288$ parameters. While I have managed to produce models with $2$ digit parameter count (the lowest at most $89$) getting cosistent perfect accuracy on 'good' data, on test set reaching $99.99\\%$ was possible, but on the adv. test it seemed difficult getting above $99.70\\%$ accuracy.","metadata":{}}]}